{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0c6b14",
   "metadata": {},
   "source": [
    "# Data science automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd8a9b",
   "metadata": {},
   "source": [
    "This week is all about looking at automation tehcniques for data science and with Python. We can automate a lot of things with Python: collecting data, processing it, cleaning it, and many other parts of the data science pipeline. \n",
    "\n",
    "For the purposes for our lecture this week, we will show how to:\n",
    "\n",
    "- use the TPOT autoML Python package to find an optimized ML model for our diabetes dataset\n",
    "- create a Python script to ingest new data and make predictions on it\n",
    "\n",
    "Often, next steps in fully operationalizing an ML pipeline like this are to use a cloud service to scale and serve our ML algorithm. We can use things like AWS lambda, GCP, AWS, or Azure ML depolyment with tools such as docker and kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ebb04-2937-4c7c-aa43-d39ffd106473",
   "metadata": {},
   "source": [
    "## Auto ML package: TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96b377-16c1-4732-a3ee-8c29206ccf1d",
   "metadata": {},
   "source": [
    "**What is TPOT?**\n",
    "\n",
    ">**TPOT** [link](http://epistasislab.github.io/tpot/latest/) is meant to be an assistant that gives you ideas on how to solve a particular machine learning problem by exploring pipeline configurations that you might have never considered, then leaves the fine-tuning to more constrained parameter tuning techniques such as grid search.\n",
    "\n",
    "Automated machine learning doesn’t replace the data scientist, (at least not yet) but it might be able to help you find good models faster. TPOT bills itself as your Data Science Assistant.\n",
    "\n",
    "TPOT helps you find good algorithms. Note that it isn’t designed for automating deep learning — something like AutoKeras might be helpful there.\n",
    "\n",
    "\n",
    "TPOT is built on the scikit learn library and follows the scikit learn API closely. It can be used for regression and classification tasks and has special implementations for medical research.\n",
    "\n",
    "TPOT is open source, well documented, and under active development. It’s development was spearheaded by researchers at the University of Pennsylvania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c00c2-6eab-4aeb-86e3-fe683ff3a3b5",
   "metadata": {},
   "source": [
    "**How does TPOT work?**\n",
    "\n",
    "TPOT has what its developers call a genetic search algorithm to find the best parameters and model ensembles. It could also be thought of as a natural selection or evolutionary algorithm. TPOT tries a pipeline, evaluates its performance, and randomly changes parts of the pipeline in search of better performing algorithms.\n",
    "\n",
    ">AutoML algorithms aren’t as simple as fitting one model on the dataset; they are considering multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with multiple preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyperparameters for all of the models and preprocessing steps, as well as multiple ways to ensemble or stack the algorithms within the pipeline.\n",
    "\n",
    "\n",
    "This power of TPOT comes from evaluating all kinds of possible pipelines automatically and efficiently. Doing this manually is cumbersome and slower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94237e8b-9d39-4fb6-b78b-e0379bc40b44",
   "metadata": {},
   "source": [
    "**Are there disadvantages to using an AutoML package?**\n",
    "\n",
    "The biggest concern with AutoML packages, such as TPOT, is the length of training time.\n",
    "\n",
    "So, how long does TPOT take to run? The short answer is that it depends.\n",
    "\n",
    "TPOT was designed to run for a while — hours or even a day. Although less complex problems with smaller datasets can see great results in minutes. You can adjust several parameters for TPOT to finish its searches faster, but at the expense of a less thorough search for an optimal pipeline. It was not designed to be a comprehensive search of preprocessing steps, feature selection, algorithms, and parameters, but it can come close if you set its parameters to be more exhaustive.\n",
    "\n",
    ">…TPOT will take a while to run on larger datasets, but it’s important to realize why. With the default TPOT settings (100 generations with 100 population size), TPOT will evaluate 10,000 pipeline configurations before finishing. To put this number into context, think about a grid search of 10,000 hyperparameter combinations for a machine learning algorithm and how long that grid search will take. That is 10,000 model configurations to evaluate with 10-fold cross-validation, which means that roughly 100,000 models are fit and evaluated on the training data in one grid search.\n",
    "\n",
    "An important TPOT parameter to set is the number of generations (via the generations kwarg). Since our aim is to just illustrate the use of TPOT, we assume the default setting of 100 generations, whilst bounding the total running time via the max_time_mins kwarg (which may, essentially, override the former setting). Further, we enable control for the maximum amount of time allowed for optimization of a single pipeline, via max_eval_time_mins.\n",
    "\n",
    "On a standard laptop with 4GB RAM, each generation takes approximately 5 minutes to run. Thus, for the default value of 100, without the explicit duration bound, the total run time could be roughly around 8 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b92a2-d3ba-4f81-9fb5-c1ea2b963d5b",
   "metadata": {},
   "source": [
    "<b>Installation: TPOT</b> <br>\n",
    "You will need to install/update the following python packages in order for TPOT to work properly:<br>\n",
    "    * pip install numpy scipy scikit-learn pandas joblib torch torchvision torchaudio pytorch <br>\n",
    "    * pip install deap update_checker tqdm stopit xgboost <br>\n",
    "    * pip install tpot **OR** conda install -c conda-forge tpot <br>\n",
    "    \n",
    "Additional details for installing extended TPOT functionality can be found on the [TPOT documentation](https://epistasislab.github.io/tpot/examples/) pages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628debe",
   "metadata": {},
   "source": [
    "# Before we get started lets build a python environment.  \n",
    "\n",
    "Creating a Python environment (usually a virtual environment) is a best practice for Python development, primarily to isolate project dependencies, prevent conflicts, and ensure projects are reproducible and portable. We can send the entire environment to another team or development environment and ensure compatibility.  It is best to create the environment in VSCode\n",
    "\n",
    "\n",
    "Steps to Create the Environment\n",
    "1. Open the Command Palette: Press Ctrl+Shift+P (Windows/Linux) or Cmd+Shift+P (macOS).\n",
    "2. Run the Create Environment Command:\n",
    "    - Start typing Python: Create Environment in the command palette search bar.\n",
    "    - Select the command when it appears.\n",
    "3. Choose the Environment Type:\n",
    "    - Select Venv from the options presented.\n",
    "4. Select a Python Interpreter:\n",
    "    - A list of available Python interpreters detected on your system will appear. Select the one you want to use for this specific project.\n",
    "5. Wait for Creation:\n",
    "    - VS Code will create a new folder, typically named .venv, within your project directory. A notification will show the progress.\n",
    "    - The extension will automatically select this new environment as the active interpreter for your workspace. \n",
    "6. Open the terminal window in VSCode and install all the necessary packages again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc02211-5e96-45aa-bbc1-28f58b457f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install numpy scipy scikit-learn pandas torch torchvision torchaudio pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d4510-7f13-4558-ad8c-ee93e20f6071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install deap update_checker tqdm stopit xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b65774-c9f0-4213-8ced-77c6dcf04036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install tpot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15620531-0a28-4936-90d5-d16d705c663f",
   "metadata": {},
   "source": [
    "## Using TPOT with our Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285173c6-12d9-4781-8167-4e302ad48d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import tpot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "import timeit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2992b8c",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ff9ce",
   "metadata": {},
   "source": [
    "First, we are going to load our same prepared data from week 2 where everything has been converted to numbers. Many autoML packages can handle non-numeric data (they usually convert it to numeric with various methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b43423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute '_format_flat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/IPython/core/formatters.py:406\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    404\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/core/frame.py:1175\u001b[0m, in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m if get_option(\"display.width\") is not None or console.in_ipython_frontend():\n\u001b[1;32m   1173\u001b[0m     # check at least the column row for excessive width\n\u001b[1;32m   1174\u001b[0m     max_rows = 1\n\u001b[0;32m-> 1175\u001b[0m else:\n\u001b[1;32m   1176\u001b[0m     max_rows = get_option(\"display.max_rows\")\n\u001b[1;32m   1178\u001b[0m # when auto-detecting, so width=None and not in ipython front end\n\u001b[1;32m   1179\u001b[0m # check whether repr fits horizontal by actually checking\n\u001b[1;32m   1180\u001b[0m # the width of the rendered repr\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/io/formats/format.py:1074\u001b[0m, in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border, table_id, render_links)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/io/formats/html.py:88\u001b[0m, in \u001b[0;36mHTMLFormatter.to_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_string\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines):\n\u001b[1;32m     90\u001b[0m         lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines]\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/io/formats/html.py:644\u001b[0m, in \u001b[0;36mNotebookFormatter.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<div>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_style()\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</div>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melements\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/io/formats/html.py:94\u001b[0m, in \u001b[0;36mHTMLFormatter.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_show_dimensions:\n\u001b[1;32m     97\u001b[0m         by \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m215\u001b[39m)  \u001b[38;5;66;03m# ×  # noqa: RUF003\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/io/formats/html.py:267\u001b[0m, in \u001b[0;36mHTMLFormatter._write_table\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<table\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mborder_attr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_classes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_section\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    263\u001b[0m     indent,\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mheader \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_body(indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent_delta)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</table>\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent)\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/io/formats/html.py:403\u001b[0m, in \u001b[0;36mHTMLFormatter._write_header\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<thead>\u001b[39m\u001b[38;5;124m\"\u001b[39m, indent)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mheader:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_col_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent_delta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_row_idx_names:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_row_header(indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent_delta)\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/io/formats/html.py:383\u001b[0m, in \u001b[0;36mHTMLFormatter._write_col_header\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m row\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_columns_formatted_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    384\u001b[0m align \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mjustify\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_truncated_horizontally:\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/pandas/io/formats/html.py:611\u001b[0m, in \u001b[0;36mNotebookFormatter._get_columns_formatted_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_columns_formatted_values\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;66;03m# only reached with non-Multi Index\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_flat\u001b[49m(include_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute '_format_flat'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                Cholesterol  Glucose  HDL Chol  Age  Gender  Height  Weight  \\\n",
       "Patient number                                                                \n",
       "1                       193       77        49   19  female      61     119   \n",
       "2                       146       79        41   19  female      60     135   \n",
       "3                       217       75        54   20  female      67     187   \n",
       "4                       226       97        70   20  female      64     114   \n",
       "5                       164       91        67   20  female      70     141   \n",
       "...                     ...      ...       ...  ...     ...     ...     ...   \n",
       "386                     227      105        44   83  female      59     125   \n",
       "387                     226      279        52   84  female      60     192   \n",
       "388                     301       90       118   89  female      61     115   \n",
       "389                     232      184       114   91  female      61     127   \n",
       "390                     165       94        69   92  female      62     217   \n",
       "\n",
       "                 BMI  Systolic BP  Diastolic BP  waist  hip     Diabetes  \n",
       "Patient number                                                            \n",
       "1               22.5          118            70     32   38  No diabetes  \n",
       "2               26.4          108            58     33   40  No diabetes  \n",
       "3               29.3          110            72     40   45  No diabetes  \n",
       "4               19.6          122            64     31   39  No diabetes  \n",
       "5               20.2          122            86     32   39  No diabetes  \n",
       "...              ...          ...           ...    ...  ...          ...  \n",
       "386             25.2          150            90     35   40  No diabetes  \n",
       "387             37.5          144            88     41   48     Diabetes  \n",
       "388             21.7          218            90     31   41  No diabetes  \n",
       "389             24.0          170            82     35   38     Diabetes  \n",
       "390             39.7          160            82     51   51  No diabetes  \n",
       "\n",
       "[390 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('diabetes_data.xlsx', index_col='Patient number')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dc97075-2488-435d-a04c-47129bd9fbd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 390 entries, 1 to 390\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Cholesterol   390 non-null    int64  \n",
      " 1   Glucose       390 non-null    int64  \n",
      " 2   HDL Chol      390 non-null    int64  \n",
      " 3   Age           390 non-null    int64  \n",
      " 4   Gender        390 non-null    object \n",
      " 5   Height        390 non-null    int64  \n",
      " 6   Weight        390 non-null    int64  \n",
      " 7   BMI           390 non-null    float64\n",
      " 8   Systolic BP   390 non-null    int64  \n",
      " 9   Diastolic BP  390 non-null    int64  \n",
      " 10  waist         390 non-null    int64  \n",
      " 11  hip           390 non-null    int64  \n",
      " 12  Diabetes      390 non-null    object \n",
      "dtypes: float64(1), int64(10), object(2)\n",
      "memory usage: 42.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96548a09-1112-4526-9621-fcfb6e0e118f",
   "metadata": {},
   "source": [
    "### Splitting our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ea51a-6e38-4e91-a25b-79041418acdb",
   "metadata": {},
   "source": [
    "As we've already seen in the prior couple of week, we need to split our dataset into feature/target sets and then into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc14d74c-f283-4940-832a-dfee259aedc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = df.drop('Diabetes', axis=1)\n",
    "targets = df['Diabetes']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, stratify=targets, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff9257-f5e7-49da-9743-15465ad314fc",
   "metadata": {},
   "source": [
    "### Using TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b8b68-cf7a-4cc2-8bda-7b9d2cba41d6",
   "metadata": {},
   "source": [
    "Most of the following should look really familar at this point.  We will:\n",
    "* establish an instance of the model\n",
    "* fit the model with our data\n",
    "* evaluate our data\n",
    "\n",
    "Also the `%%time` command is telling Jupyter to provide us with length of run information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba921e74-5c0b-4871-83ec-4ed9dc5173ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taniaaguilar/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/tpot/tpot_estimator/estimator.py:458: UserWarning: Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\n",
      "  warnings.warn(\"Both generations and max_time_mins are set. TPOT will terminate when the first condition is met.\")\n",
      "/Users/taniaaguilar/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/distributed/node.py:188: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 55229 instead\n",
      "  warnings.warn(\n",
      "/Users/taniaaguilar/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/tpot/tpot_estimator/estimator.py:537: UserWarning: Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\n",
      "  warnings.warn(\"Labels are not encoded as ints from 0 to N. For compatibility with some classifiers such as sklearn, TPOT has encoded y with the sklearn LabelEncoder. When using pipelines outside the main TPOT estimator class, you can encode the labels with est.label_encoder_\")\n",
      "Generation:   0%|          | 0/5 [01:21<?, ?it/s]\n",
      "Generation:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose>=4 to see the errors that caused individuals to fail.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/tpot/tpot_estimator/templates/tpottemplates.py:568\u001b[0m, in \u001b[0;36mTPOTClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28msuper\u001b[39m(TPOTClassifier,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    541\u001b[0m         search_space\u001b[38;5;241m=\u001b[39msearch_space,\n\u001b[1;32m    542\u001b[0m         scorers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorers, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtpotestimator_kwargs)\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/tpot/tpot_estimator/estimator.py:766\u001b[0m, in \u001b[0;36mTPOTEstimator.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evolver_instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evolver_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evolver(   individual_generator\u001b[38;5;241m=\u001b[39mind_generator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng),\n\u001b[1;32m    721\u001b[0m                                     objective_functions\u001b[38;5;241m=\u001b[39m [objective_function],\n\u001b[1;32m    722\u001b[0m                                     objective_function_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective_function_weights,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    762\u001b[0m                                     rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng,\n\u001b[1;32m    763\u001b[0m                                     )\n\u001b[0;32m--> 766\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evolver_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m#self._evolver_instance.population.update_pareto_fronts(self.objective_names, self.objective_function_weights)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_evaluated_individuals()\n",
      "File \u001b[0;32m~/Desktop/Desktop - Tania’s MacBook Air/Regis University/Intro to DS/HW/Week 5/.venv/lib/python3.10/site-packages/tpot/evolvers/base_evolver.py:506\u001b[0m, in \u001b[0;36mBaseEvolver.optimize\u001b[0;34m(self, generations)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_population()\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation\u001b[38;5;241m.\u001b[39mpopulation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose>=4 to see the errors that caused individuals to fail.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# Generation 1 is the first generation after the initial population\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mException\u001b[0m: No individuals could be evaluated in the initial population. This may indicate a bug in the configuration, included models, or objective functions. Set verbose>=4 to see the errors that caused individuals to fail."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tpot_model = tpot.TPOTClassifier(generations=5, population_size=50, cv=5,random_state=42)\n",
    "\n",
    "tpot_model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcccf8bb-05b1-499f-b29f-1ff7f85b4f5c",
   "metadata": {},
   "source": [
    "Did you notice the one difference from our prior machine learning materials?\n",
    "\n",
    "Yup! We are not evaluating the performance of our model against our training data this time. TPOT does that for us.\n",
    "For more details on the individual parameters we are using, take a look at the [TPOT documentation](https://epistasislab.github.io/tpot/examples/) pages.\n",
    "\n",
    "Now we can use the TPOT model to make predictions for our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12eacf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_model.evaluated_individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1556ad39",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtpot_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpareto_front\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInstance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "tpot_model.pareto_front['Instance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = get_scorer(\"accuracy\")\n",
    "scorer(tpot_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d6cb1-34a0-4427-a784-2bd1750d779a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = tpot_model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac0a29-e180-40ec-9e1f-bcbfb664c40b",
   "metadata": {},
   "source": [
    "Let's compare our TPOT's predictions against the actuals for the test dataseet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5eed1f-a13f-493b-89d8-2a1d7041231a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the actuals and predictions for the test set\n",
    "print('Predictions for test data set')\n",
    "print(predictions)\n",
    "print('Actuals for test data set')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ef260-38bd-4afc-bc3d-40e4c8553660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Accuracy of the TPOT predictions: {accuracy_score(y_test,predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c17110",
   "metadata": {},
   "source": [
    "### Saving our TPOT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47095f",
   "metadata": {},
   "source": [
    "Next, we want to save our trained model so we can use it in a Python file later. Afterall, having a model that is only available on your local machine isn't idea in a corporate setting. You will want to by able to deploy your model into a production environment. \n",
    "\n",
    "We will use Pickel to save our model and then run a the predict_diabetes.py python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098bc67-7b4f-41cd-bb62-e9a67eefd605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the fitted pipeline\n",
    "with open('tpot_diabetes_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(tpot_model.fitted_pipeline_, f)\n",
    "\n",
    "# Later, load and use it\n",
    "with open('tpot_diabetes_pipeline.pkl', 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)\n",
    "    predictions = loaded_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0934bf-a5d8-4fcb-997a-4dad27b75a4c",
   "metadata": {},
   "source": [
    "Yup! That's all there is to it. Let's take a look at the file that was created.\n",
    "\n",
    "We can now use this model in a Python file to take in new data and make a prediction. We will first need to compose a Python file. We can do this in many ways:\n",
    "\n",
    "- Jupyter and Jupyter Lab\n",
    "- VS Code\n",
    "- Atom\n",
    "- Notepad++/Sublime\n",
    "- Other text editors or IDEs (integrated development environments)\n",
    "\n",
    "The benefit of using a code editor or IDE is that it will have lots of bells and whistles, like syntax highlighting, autocomplete, and many other things depending on the code editor or IDE. VS Code is one of the top-most used editors by data scientists and software developers, although you can try any IDE or code editor for Python that you like. You can easily install VS Code through Anaconda Navigator or by visiting the VS Code website. VS Code is developed by Microsoft, and there is also an IDE Visual Studio Code.\n",
    "\n",
    "The file we've created is show below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314d795-0964-49a4-bc7b-861e4b5ee5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Code\n",
    "\n",
    "Code('predict_diabetes.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17f368-9039-4710-896e-6a31d6f70f87",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important::</b> The code above will not execute as-is. \n",
    "    \n",
    "The following comment is specifying assumptions about the format of your dataset. You would need to provide code to ensure that the incoming dataset met this format <b> OR </b> change the code to reflect your existing dataset format. <br>\n",
    "&emsp;\\# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "\n",
    "Also, notice the location of the dataset is not specified below. Generally in a production situation, file locations are passed by a parameter.  Since TPOT doesn't know this information, you have been provided with a template that you need to adjust to fit your environment. <br>\n",
    "&emsp;tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa7348",
   "metadata": {},
   "source": [
    "We can test out running the file with the Jupyter \"magic\" command %run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb992bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run predict_diabetes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf980159-32bd-426e-9072-7c381b34bff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff7ff9",
   "metadata": {},
   "source": [
    "# Saving our code to GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8d60d",
   "metadata": {},
   "source": [
    "The last few things to do are to write a short summary of our process and results and create a GitHub repository and upload our code there. If you don't already have an account, head over to github.com and create one. Then, you need to install a GitHub client on your computer. Definitely the easiest way is to use the [GUI](https://desktop.github.com/), although if you're more advanced or adventurous you can use the CLI instead.  GitHub is also run and owned by Microsoft (they bought it in 2018).\n",
    "\n",
    "Once you have an account and GitHub installed, you can create a new repository, either through the GUI with File -> New repository or through the web interface. It's best to select the option 'Initialize this repository with a README' and 'Python' for the 'Git ignore' option. The Git ignore option creates a file that ignores common files that we don't need that are related to Python (like our Jupyter Notebook checkpoints folders). Lastly, it's not a bad idea to choose a license. The MIT license is a a very open and open-source license, although others are more restrictive like Apache. We can choose MIT here since we aren't worried about protecting intellectual property in this case.\n",
    "\n",
    "When you publish to GitHub you need to include a README file that explains and defines the repo.  It is also good practice to include along with you codebase a `requirements.txt` file `pip freeze > requirements.txt` which will include all the packages used in the projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ca5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66351b6d",
   "metadata": {},
   "source": [
    "Once we've created the repository, we can open the folder by browsing there, with a hotkey through the GUI, or by clicking the button in the GUI to open the folder. We need to copy our work there, then write a short note in the 'summary' area, and hit 'commit to main'. The 'main' label is a branch of the Git repository - we can have several branches in parallel but 'main' is default. Then we can hit the 'push origin' button in the upper right to upload our data to the GitHub's cloud.  Last, we simply need to put the link in a text file and turn that in for our week 5 assignment (using the churn data instead of the diabetes data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7ec45",
   "metadata": {},
   "source": [
    "# Optional advanced section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6448448",
   "metadata": {},
   "source": [
    "Although we don't have walkthroughs for it this week, there are other autoML packages in Python that we can use. These have documentation with examples showing how to use them. For example, here are the docs for some of these:\n",
    "- [H2O](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)\n",
    "- [MLBox](https://mlbox.readthedocs.io/en/latest/introduction.html)\n",
    "- [LazyPredict](https://lazypredict.readthedocs.io/en/latest/)\n",
    "\n",
    "Of course, using these packages requires that you first install them. At least with H2O, using conda is easier than with pip.\n",
    "\n",
    "We can also improve our Python module by using a class instead of plain functions. We can read more about creating classes [here](https://realpython.com/python3-object-oriented-programming/). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
